{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>15001</td>\n",
       "      <td>1.178755</td>\n",
       "      <td>0.596025</td>\n",
       "      <td>0.074131</td>\n",
       "      <td>2.542393</td>\n",
       "      <td>0.450685</td>\n",
       "      <td>-0.179355</td>\n",
       "      <td>0.326365</td>\n",
       "      <td>-0.234949</td>\n",
       "      <td>0.473040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228112</td>\n",
       "      <td>-0.561559</td>\n",
       "      <td>-0.182781</td>\n",
       "      <td>-0.523241</td>\n",
       "      <td>0.667142</td>\n",
       "      <td>0.015699</td>\n",
       "      <td>-0.067238</td>\n",
       "      <td>0.008709</td>\n",
       "      <td>53.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>15004</td>\n",
       "      <td>1.228455</td>\n",
       "      <td>0.049488</td>\n",
       "      <td>1.022099</td>\n",
       "      <td>0.386471</td>\n",
       "      <td>-0.973228</td>\n",
       "      <td>-1.067822</td>\n",
       "      <td>-0.383162</td>\n",
       "      <td>-0.205407</td>\n",
       "      <td>1.699304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322912</td>\n",
       "      <td>-0.730294</td>\n",
       "      <td>0.205601</td>\n",
       "      <td>0.686302</td>\n",
       "      <td>-0.071008</td>\n",
       "      <td>0.729846</td>\n",
       "      <td>-0.092276</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>12.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>15008</td>\n",
       "      <td>-0.971734</td>\n",
       "      <td>0.744625</td>\n",
       "      <td>2.334822</td>\n",
       "      <td>-0.408046</td>\n",
       "      <td>-0.999231</td>\n",
       "      <td>-0.629294</td>\n",
       "      <td>-0.377212</td>\n",
       "      <td>0.481230</td>\n",
       "      <td>1.599496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116821</td>\n",
       "      <td>-0.141219</td>\n",
       "      <td>-0.026115</td>\n",
       "      <td>0.712719</td>\n",
       "      <td>-0.372964</td>\n",
       "      <td>0.750323</td>\n",
       "      <td>-0.107875</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>15010</td>\n",
       "      <td>-1.529666</td>\n",
       "      <td>1.475870</td>\n",
       "      <td>1.507624</td>\n",
       "      <td>-0.662935</td>\n",
       "      <td>-1.037152</td>\n",
       "      <td>-1.159860</td>\n",
       "      <td>-0.303219</td>\n",
       "      <td>0.745766</td>\n",
       "      <td>0.946896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197993</td>\n",
       "      <td>-0.634088</td>\n",
       "      <td>0.100631</td>\n",
       "      <td>0.669449</td>\n",
       "      <td>-0.269750</td>\n",
       "      <td>0.611964</td>\n",
       "      <td>-0.169789</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>15012</td>\n",
       "      <td>-1.181721</td>\n",
       "      <td>1.485264</td>\n",
       "      <td>1.958715</td>\n",
       "      <td>2.587943</td>\n",
       "      <td>-0.504092</td>\n",
       "      <td>-0.126697</td>\n",
       "      <td>0.939038</td>\n",
       "      <td>0.175638</td>\n",
       "      <td>-0.756318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010641</td>\n",
       "      <td>-0.191361</td>\n",
       "      <td>0.204004</td>\n",
       "      <td>0.663928</td>\n",
       "      <td>0.288780</td>\n",
       "      <td>-0.033612</td>\n",
       "      <td>-0.142682</td>\n",
       "      <td>0.028149</td>\n",
       "      <td>159.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0         0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1         0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2         1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3         1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4         2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  15001  1.178755  0.596025  0.074131  2.542393  0.450685 -0.179355   \n",
       "9996  15004  1.228455  0.049488  1.022099  0.386471 -0.973228 -1.067822   \n",
       "9997  15008 -0.971734  0.744625  2.334822 -0.408046 -0.999231 -0.629294   \n",
       "9998  15010 -1.529666  1.475870  1.507624 -0.662935 -1.037152 -1.159860   \n",
       "9999  15012 -1.181721  1.485264  1.958715  2.587943 -0.504092 -0.126697   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1    -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2     0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3     0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4     0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.326365 -0.234949  0.473040  ... -0.228112 -0.561559 -0.182781   \n",
       "9996 -0.383162 -0.205407  1.699304  ... -0.322912 -0.730294  0.205601   \n",
       "9997 -0.377212  0.481230  1.599496  ... -0.116821 -0.141219 -0.026115   \n",
       "9998 -0.303219  0.745766  0.946896  ... -0.197993 -0.634088  0.100631   \n",
       "9999  0.939038  0.175638 -0.756318  ... -0.010641 -0.191361  0.204004   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1    -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2    -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3    -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4     0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "...        ...       ...       ...       ...       ...     ...    ...  \n",
       "9995 -0.523241  0.667142  0.015699 -0.067238  0.008709   53.19      0  \n",
       "9996  0.686302 -0.071008  0.729846 -0.092276  0.012277   12.18      0  \n",
       "9997  0.712719 -0.372964  0.750323 -0.107875  0.031272    4.05      0  \n",
       "9998  0.669449 -0.269750  0.611964 -0.169789  0.007846    4.05      0  \n",
       "9999  0.663928  0.288780 -0.033612 -0.142682  0.028149  159.28      0  \n",
       "\n",
       "[10000 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_fraud=pd.read_csv(\"credit_card.csv\")\n",
    "df_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the null values\n",
    "df_fraud.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5966.033400</td>\n",
       "      <td>-0.241862</td>\n",
       "      <td>0.281949</td>\n",
       "      <td>0.906270</td>\n",
       "      <td>0.264148</td>\n",
       "      <td>-0.046398</td>\n",
       "      <td>0.133108</td>\n",
       "      <td>-0.071689</td>\n",
       "      <td>-0.064778</td>\n",
       "      <td>0.802224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>-0.152671</td>\n",
       "      <td>-0.033268</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>63.030188</td>\n",
       "      <td>0.00380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4473.403739</td>\n",
       "      <td>1.521679</td>\n",
       "      <td>1.308139</td>\n",
       "      <td>1.159154</td>\n",
       "      <td>1.441235</td>\n",
       "      <td>1.182935</td>\n",
       "      <td>1.307311</td>\n",
       "      <td>1.077430</td>\n",
       "      <td>1.259064</td>\n",
       "      <td>1.155198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913811</td>\n",
       "      <td>0.631083</td>\n",
       "      <td>0.487814</td>\n",
       "      <td>0.594430</td>\n",
       "      <td>0.428171</td>\n",
       "      <td>0.562793</td>\n",
       "      <td>0.410868</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>184.486158</td>\n",
       "      <td>0.06153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-27.670569</td>\n",
       "      <td>-34.607649</td>\n",
       "      <td>-15.496222</td>\n",
       "      <td>-4.657545</td>\n",
       "      <td>-32.092129</td>\n",
       "      <td>-23.496714</td>\n",
       "      <td>-26.548144</td>\n",
       "      <td>-23.632502</td>\n",
       "      <td>-6.329801</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.468435</td>\n",
       "      <td>-8.527145</td>\n",
       "      <td>-15.144340</td>\n",
       "      <td>-2.512377</td>\n",
       "      <td>-2.577363</td>\n",
       "      <td>-1.338556</td>\n",
       "      <td>-7.976100</td>\n",
       "      <td>-3.509250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2072.750000</td>\n",
       "      <td>-1.013283</td>\n",
       "      <td>-0.208342</td>\n",
       "      <td>0.412799</td>\n",
       "      <td>-0.614424</td>\n",
       "      <td>-0.643390</td>\n",
       "      <td>-0.629934</td>\n",
       "      <td>-0.542336</td>\n",
       "      <td>-0.190747</td>\n",
       "      <td>0.070868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268120</td>\n",
       "      <td>-0.549638</td>\n",
       "      <td>-0.174120</td>\n",
       "      <td>-0.327817</td>\n",
       "      <td>-0.158137</td>\n",
       "      <td>-0.327974</td>\n",
       "      <td>-0.084489</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4563.500000</td>\n",
       "      <td>-0.372799</td>\n",
       "      <td>0.288524</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>0.219861</td>\n",
       "      <td>-0.152769</td>\n",
       "      <td>-0.152566</td>\n",
       "      <td>-0.055585</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.805275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123273</td>\n",
       "      <td>-0.136746</td>\n",
       "      <td>-0.045794</td>\n",
       "      <td>0.079976</td>\n",
       "      <td>0.121001</td>\n",
       "      <td>0.042865</td>\n",
       "      <td>-0.004568</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>15.950000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10233.250000</td>\n",
       "      <td>1.150864</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>1.602903</td>\n",
       "      <td>1.125666</td>\n",
       "      <td>0.371081</td>\n",
       "      <td>0.505357</td>\n",
       "      <td>0.476280</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>1.506299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.247490</td>\n",
       "      <td>0.081665</td>\n",
       "      <td>0.410877</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.476394</td>\n",
       "      <td>0.120811</td>\n",
       "      <td>0.077182</td>\n",
       "      <td>50.960000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15012.000000</td>\n",
       "      <td>1.960497</td>\n",
       "      <td>8.636214</td>\n",
       "      <td>4.101716</td>\n",
       "      <td>10.463020</td>\n",
       "      <td>34.099309</td>\n",
       "      <td>21.393069</td>\n",
       "      <td>34.303177</td>\n",
       "      <td>5.060381</td>\n",
       "      <td>10.392889</td>\n",
       "      <td>...</td>\n",
       "      <td>22.588989</td>\n",
       "      <td>4.534454</td>\n",
       "      <td>13.876221</td>\n",
       "      <td>3.200201</td>\n",
       "      <td>5.525093</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>8.254376</td>\n",
       "      <td>4.860769</td>\n",
       "      <td>7712.430000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time            V1            V2            V3            V4  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean    5966.033400     -0.241862      0.281949      0.906270      0.264148   \n",
       "std     4473.403739      1.521679      1.308139      1.159154      1.441235   \n",
       "min        0.000000    -27.670569    -34.607649    -15.496222     -4.657545   \n",
       "25%     2072.750000     -1.013283     -0.208342      0.412799     -0.614424   \n",
       "50%     4563.500000     -0.372799      0.288524      0.944361      0.219861   \n",
       "75%    10233.250000      1.150864      0.901879      1.602903      1.125666   \n",
       "max    15012.000000      1.960497      8.636214      4.101716     10.463020   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.046398      0.133108     -0.071689     -0.064778      0.802224   \n",
       "std        1.182935      1.307311      1.077430      1.259064      1.155198   \n",
       "min      -32.092129    -23.496714    -26.548144    -23.632502     -6.329801   \n",
       "25%       -0.643390     -0.629934     -0.542336     -0.190747      0.070868   \n",
       "50%       -0.152769     -0.152566     -0.055585      0.012865      0.805275   \n",
       "75%        0.371081      0.505357      0.476280      0.274533      1.506299   \n",
       "max       34.099309     21.393069     34.303177      5.060381     10.392889   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   ...     -0.051990     -0.152671     -0.033268      0.021335   \n",
       "std    ...      0.913811      0.631083      0.487814      0.594430   \n",
       "min    ...    -11.468435     -8.527145    -15.144340     -2.512377   \n",
       "25%    ...     -0.268120     -0.549638     -0.174120     -0.327817   \n",
       "50%    ...     -0.123273     -0.136746     -0.045794      0.079976   \n",
       "75%    ...      0.032707      0.247490      0.081665      0.410877   \n",
       "max    ...     22.588989      4.534454     13.876221      3.200201   \n",
       "\n",
       "                V25           V26           V27           V28        Amount  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.087146      0.108140      0.005518      0.002915     63.030188   \n",
       "std        0.428171      0.562793      0.410868      0.266247    184.486158   \n",
       "min       -2.577363     -1.338556     -7.976100     -3.509250      0.000000   \n",
       "25%       -0.158137     -0.327974     -0.084489     -0.015753      5.000000   \n",
       "50%        0.121001      0.042865     -0.004568      0.015897     15.950000   \n",
       "75%        0.359058      0.476394      0.120811      0.077182     50.960000   \n",
       "max        5.525093      3.517346      8.254376      4.860769   7712.430000   \n",
       "\n",
       "             Class  \n",
       "count  10000.00000  \n",
       "mean       0.00380  \n",
       "std        0.06153  \n",
       "min        0.00000  \n",
       "25%        0.00000  \n",
       "50%        0.00000  \n",
       "75%        0.00000  \n",
       "max        1.00000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets describe the dataset\n",
    "df_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_fraud[df_fraud.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets check the count of each class in target column\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x=df_fraud.columns[-1],data=df_fraud)\n",
    "plt.show()\n",
    "#from below its obvious that it is an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the skewness of the dataset\n",
    "len(df_fraud.skew().loc[(df_fraud.skew()>0.55) | (df_fraud.skew()<-0.55)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets seprate the input and output variable\n",
    "df_x=df_fraud.iloc[:,:-1]\n",
    "y=df_fraud.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets treat the skewness\n",
    "import numpy as np\n",
    "for index in df_x.skew().index:\n",
    "    if df_x.skew().loc[index]>0.55:\n",
    "        df_x[index]=np.cbrt(df_x[index])\n",
    "    if df_x.skew().loc[index]<-0.55:\n",
    "        df_x[index]=np.cbrt(df_x[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets again check the skewness\n",
    "len(df_x.skew().loc[(df_x.skew()>0.55) | (df_x.skew()<-0.55)])\n",
    "#From output it is clear that number of skewed variables reduced significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we know it is highly imbalanced data and needs to be treated\n",
    "#Lets use SMOTE to oversample minority class\n",
    "from imblearn.over_sampling import SMOTE\n",
    "df_x, y = SMOTE().fit_sample(df_x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.88258485, -0.73863   , -1.44081218, ...,  0.34333641,\n",
       "        -1.75391922,  1.60717127],\n",
       "       [-1.88258485,  1.38998506, -0.26532963, ..., -0.76129657,\n",
       "         0.25379896, -0.48102596],\n",
       "       [-1.88233633, -0.73824243, -2.19989861, ..., -1.02742142,\n",
       "        -1.87973404,  2.63357895],\n",
       "       ...,\n",
       "       [ 1.01471362, -1.20150727,  1.1142343 , ...,  1.22200276,\n",
       "         0.81508438, -0.72317625],\n",
       "       [-0.1160728 , -0.84801277,  0.37467467, ..., -1.24032853,\n",
       "         0.79476417,  0.50176105],\n",
       "       [ 0.88995433, -0.65341362,  0.80192107, ...,  1.45568641,\n",
       "         0.77213389, -0.68928646]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets put all features to common scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(df_x)\n",
    "x=sc.transform(df_x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def max_aucroc_score(clf,df_x,y):\n",
    "    max_aucroc_score=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x, y,random_state = r_state,test_size=0.20,stratify=y)\n",
    "        #x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "        clf.fit(x_train,y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        aucroc_scr=roc_auc_score(y_test,y_pred)\n",
    "       # print(\"auc roc score corresponding to \",r_state,\" is \",aucroc_scr)\n",
    "        if aucroc_scr>max_aucroc_score:\n",
    "            max_aucroc_score=aucroc_scr\n",
    "            final_r_state=r_state\n",
    "    print(\"max auc roc score corresponding to \",final_r_state,\" is \",max_aucroc_score)\n",
    "    return final_r_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max auc roc score corresponding to  47  is  1.0\n"
     ]
    }
   ],
   "source": [
    "#Lets use logistic regression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "r_state=max_aucroc_score(logreg,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"n_estimators\":[10,100,500]}\n",
    "rf_clf=RandomForestClassifier()\n",
    "clf = GridSearchCV(rf_clf, parameters, cv=5)\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max auc roc score corresponding to  42  is  1.0\n"
     ]
    }
   ],
   "source": [
    "#lets chcek max auc roc score when we use random forest\n",
    "rf_clf=RandomForestClassifier(n_estimators=100)\n",
    "r_state=max_aucroc_score(rf_clf,df_x,y111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19924, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets use Gradient boosting classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "parameters={\"learning_rate\":[0.01,0.1,1],\"n_estimators\":[10,100,500]}\n",
    "gb_clf=GradientBoostingClassifier()\n",
    "clf = GridSearchCV(gb_clf, parameters, cv=5)\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max auc roc score corresponding to  44  is  1.0\n"
     ]
    }
   ],
   "source": [
    "#lets chcek max auc roc score when we using gradient boosting\n",
    "gb_clf=GradientBoostingClassifier(learning_rate=1,n_estimators=100)\n",
    "r_state=max_aucroc_score(gb_clf,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean auc roc score for random forest classifier:  0.9999999495975871\n",
      "standard deviation in auc roc score for random forest classifier:  0.0\n",
      "\n",
      "Mean auc roc score for gradient boosting classifier:  0.9995957728258563\n",
      "standard deviation in auc roc score for gradient boosting classifier:  0.0008076735559029348\n"
     ]
    }
   ],
   "source": [
    "#Lets use cross val score to compare two algorithms\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"Mean auc roc score for random forest classifier: \",cross_val_score(rf_clf,df_x,y,cv=5,scoring=\"roc_auc\").mean())\n",
    "print(\"standard deviation in auc roc score for random forest classifier: \",cross_val_score(rf_clf,df_x,y,cv=5,scoring=\"roc_auc\").std())\n",
    "print()\n",
    "print(\"Mean auc roc score for gradient boosting classifier: \",cross_val_score(gb_clf,df_x,y,cv=5,scoring=\"roc_auc\").mean())\n",
    "print(\"standard deviation in auc roc score for gradient boosting classifier: \",cross_val_score(gb_clf,df_x,y,cv=5,scoring=\"roc_auc\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on above output clearly random forest classifier is performing better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since from above we know maximum auc roc score we got for random state 42 so we will use it\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, y,random_state = 42,test_size=0.20,stratify=y)\n",
    "rf_clf=RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(x_train,y_train)\n",
    "y_pred=rf_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[1992    1]\n",
      " [   0 1992]]\n",
      "auc roc score is :  0.9997491219267436\n"
     ]
    }
   ],
   "source": [
    "#Lets print auc roc score and confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"auc roc score is : \",roc_auc_score(y_test,y_pred))\n",
    "#so the best what we can get we got here and using above cross validation we verified that its not result of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets plot AUC-ROC CURVE\n",
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = rf_clf.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
